{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "# to download from a particular website\n",
    "import urllib\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the text corpus\n",
    "url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "data_path = 'text8.zip'\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading the dataset...\")\n",
    "    filename, _ = urllib.request.urlretrieve(url, data_path)\n",
    "    print(\"Done!\")\n",
    "# unzip the file and read the dataset mfile by file and append inside a list \n",
    "with zipfile.ZipFile(data_path) as f:\n",
    "    # f.namelist() reads full filepath of files and first element is the filename\n",
    "    text_words = f.read(f.namelist()[0]).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count reverse by steps and endpoint\n",
    "# for i in range(16 - 1, -1, -1):\n",
    "#     print(i)\n",
    "# example = dict()\n",
    "# example['mouse'] = 1\n",
    "# example['keyboard'] = 2\n",
    "# example['CPU'] = 3\n",
    "# example.get('CPUS', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need a max vocab size to limit the vocab\n",
    "max_vocab_size = 50000\n",
    "# extend adds to a list as it is described and append decouples individual elements and adds to the element\n",
    "count = [('UNK', -1)]\n",
    "# https://www.hackerrank.com/challenges/collections-counter/problem to understand how counter works\n",
    "count.extend(collections.Counter(text_words).most_common(max_vocab_size - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str_count = list()\n",
    "for i in range(len(count) - 1):\n",
    "    if i !=0 :\n",
    "        str_count.append((count[i][0].decode(\"utf-8\"), count[i][1]))\n",
    "    else:\n",
    "        str_count.append((count[i][0], count[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count:  17005207\n",
      "unique word count:  253854\n",
      "vocab size:  46984\n",
      "top 10 words:  [('UNK', 17005207), ('zero', 264975), ('nine', 250430), ('two', 192644), ('eight', 125285), ('five', 115789), ('three', 114775), ('four', 108182), ('six', 102145), ('seven', 99683)]\n"
     ]
    }
   ],
   "source": [
    "# We need a min count of individual words so that we don't have to work with rare words\n",
    "min_occurrence = 10\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "# we are popping out infrequent words out of the vocab\n",
    "for i in range(len(str_count) -1, -1, -1):\n",
    "    if str_count[i][1] < min_occurrence:\n",
    "        str_count.pop(i)\n",
    "    elif str_count[i][0] in stops:\n",
    "        str_count.pop(i)\n",
    "    else:\n",
    "        pass\n",
    "# compute vocab size\n",
    "vocab_size = len(str_count)\n",
    "# assign ids to individual words\n",
    "word2id = dict()\n",
    "for i, (word, _) in enumerate(str_count):\n",
    "    word2id[word] = i\n",
    "\n",
    "data = list()\n",
    "unk_count = 0\n",
    "# retrieve a word id or assign it to index 0 ('UNK') if it's not in the dictionary\n",
    "for word in text_words:\n",
    "    # either you get the id of the word; if not found then return 0\n",
    "    index = word2id.get(word, 0)\n",
    "    if index == 0:\n",
    "        unk_count += 1\n",
    "    data.append(index)\n",
    "# changing the count of unknown words in the dictionary\n",
    "str_count[0] = ('UNK', unk_count)\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "print(\"word count: \", len(text_words))\n",
    "print(\"unique word count: \", len(set(text_words)))\n",
    "print(\"vocab size: \", vocab_size)\n",
    "print(\"top 10 words: \", str_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "skip_window = 3 # how many words to consider on left and right\n",
    "num_skips = 2 # how many times to consider an input to generate labels\n",
    "neg_samples = 64 # Number of negative examples to sample\n",
    "# to understand the parameters\n",
    "# https://stackoverflow.com/questions/47302947/understanding-input-and-labels-in-word2vec-tensorflow\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 - num_skips=4\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=4, skip_window=2)  \n",
    "It generates 4 labels for each word, i.e. uses the whole context; since batch_size=8 only 2 words are processed in this batch (12 and 6), the rest will go into the next batch:  \n",
    "\n",
    "data = [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156, 128, 742, 477, 10572, ...]  \n",
    "batch = [12 12 12 12  6  6  6  6]  \n",
    "labels = [[6 3084 5239 195 195 3084 12 2]]  \n",
    "\n",
    "#### Example 2 - num_skips=2  \n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=2)  \n",
    "Here you would expect each word appear twice in the batch sequence; the 2 labels are randomly sampled from 4 possible words:  \n",
    "\n",
    "data = [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156, 128, 742, 477, 10572, ...]  \n",
    "batch = [ 12  12   6   6 195 195   2   2]  \n",
    "labels = [[ 195 3084   12  195 3137   12   46  195]]  \n",
    "\n",
    "#### Example 3 - num_skips=1  \n",
    "batch, labels = generate_batch(batch_size=8, num_skips=1, skip_window=2)  \n",
    "Finally, this setting, same as yours, produces exactly one label per each word; each label is drawn randomly from the 4-word context:  \n",
    "\n",
    "data = [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156, 128, 742, 477, 10572, ...]  \n",
    "batch = [  12    6  195    2 3137   46   59  156]  \n",
    "labels = [[  6  12  12 195  59 156  46  46]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec global Parameters\n",
    "embedding_size = 200 # dimension of an embedding vector\n",
    "max_vocabulary_size = 50000 # total number of different words that should be in the vocabulary\n",
    "min_occurrence = 10 # remove all words that does not appears at least n=10 times\n",
    "num_sampled = 64 # number of negative examples to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0 # initialize\n",
    "# Generate training batch for the skip-gram model\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index # pointing to the out of scope variable\n",
    "    assert batch_size % num_skips == 0 # batchsize should be multiple of num_skips; see above example\n",
    "    assert num_skips <= 2 * skip_window # so one can get proper sampled labels from both side of the input\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32) # see above input\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # see above input\n",
    "    # get window size (words left and right + current one)\n",
    "    span = 2 * skip_window + 1 # verbose\n",
    "    buffer = collections.deque(maxlen=span) # double ended queue to hold the set (w_-2, w_-1, w, w_+1, w_+2)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    \"\"\"\n",
    "    setting up ip, label parallelly \n",
    "    batch = [12 12 12 12 6 6 6 6]\n",
    "    labels = [[6 3084 5239 195 195 3084 12 2]]\n",
    "    \"\"\" \n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window] # choosing all context words\n",
    "        words_to_use = random.sample(context_words, num_skips) # randomly sample context words\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window] # choosing 12, 12, 12, 12 iteratively\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word] # choosing 6, 3084, 5239, 195\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data) # so that we get all the words for training\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand 'None' shape  \n",
    "https://stackoverflow.com/questions/46940857/what-is-the-difference-between-none-none-and-for-the-shape-of-a-placeh  \n",
    "TensorFlow uses arrays rather than tuples. It converts tuples to arrays. Therefore [] and () are equivalent.  \n",
    "\n",
    "Now, consider this code example:  \n",
    "\n",
    "x = tf.placeholder(dtype=tf.int32, shape=[], name=\"foo1\")  \n",
    "y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"foo2\")  \n",
    "z = tf.placeholder(dtype=tf.int32, shape=None, name=\"foo3\")  \n",
    "\n",
    "val1 = np.array((1, 2, 3))  \n",
    "val2 = 45  \n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())  \n",
    "\n",
    "    #print(sess.run(x, feed_dict = {x: val1}))  # Fails  \n",
    "    print(sess.run(y, feed_dict = {y: val1}))  \n",
    "    print(sess.run(z, feed_dict = {z: val1}))  \n",
    "  \n",
    "    print(sess.run(x, feed_dict = {x: val2}))  \n",
    "    #print(sess.run(y, feed_dict = {y: val2}))  # Fails  \n",
    "    print(sess.run(z, feed_dict = {z: val2}))  \n",
    "As can be seen, placeholder with [] shape takes a single scalar value directly. Placeholder with [None] shape   takes a 1-dimensional array and placeholder with None shape can take in any value while computation takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NCE Loss: https://mk-minchul.github.io/NCE/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep_dims tutorial\n",
    "- a  = array([[0, 0, 0],[0, 1, 0],[0, 2, 0],[1, 0, 0],[1, 1, 0]])  \n",
    "- np.sum(a, keepdims=True) = array([[6]])  \n",
    "- np.sum(a, keepdims=False) = 6  \n",
    "- np.sum(a, axis=1, keepdims=True) = array([[0],[1],[2],[1],[2]])  \n",
    "- np.sum(a, axis=1, keepdims=False) = array([0, 1, 2, 1, 2])     \n",
    "- np.sum(a, axis=0, keepdims=True) = array([[2, 4, 0]])    \n",
    "- np.sum(a, axis=0, keepdims=False) = array([2, 4, 0])  \n",
    "\n",
    "#### transpose_b = True tutorial\n",
    "x = tf.constant([1.,2.,3.], shape = (3,2,4))  \n",
    "y = tf.constant([1.,2.,3.], shape = (3,21,4))  \n",
    "tf.matmul(x,y)                     # Doesn't work.   \n",
    "tf.matmul(x,y,transpose_b = True)  # This works. Shape is (3,2,21)  \n",
    "tf.matmul(x,tf.transpose(y))       # Doesn't work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "num_steps = 3000000\n",
    "display_step = 10000\n",
    "eval_step = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up inputs\n",
    "X = tf.placeholder(tf.int32, shape=[None])\n",
    "# setting up labels\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "with tf.device('/cpu:0'):\n",
    "    # embedding layer variable (each row represent a word embedding vector)\n",
    "    embedding = tf.Variable(tf.random_normal([vocab_size, embedding_size]))\n",
    "    # lookup for corresponding embedding\n",
    "    X_embed = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(tf.random_normal([vocab_size, embedding_size]))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "    \n",
    "# computation of average NCE loss for a batch\n",
    "loss_op = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights,\n",
    "                   biases=nce_biases,\n",
    "                   labels=Y,\n",
    "                   inputs=X_embed,\n",
    "                   num_sampled=num_sampled,\n",
    "                   num_classes=vocab_size))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# computing similarity between input and embeddings\n",
    "X_embed_norm = X_embed / tf.sqrt(tf.reduce_sum(tf.square(X_embed)))\n",
    "embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "cosine_sim_op = tf.matmul(X_embed_norm, embedding_norm, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluation parameters\n",
    "eval_words = ['five', 'man', 'going', 'hardware', 'american', 'britain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    # Testing data\n",
    "    x_test = np.array([word2id[w] for w in eval_words])\n",
    "    average_loss = 0\n",
    "    for step in range(1, num_steps + 1):\n",
    "        # Get a new batch of data\n",
    "        batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
    "        # Run training op\n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "        average_loss += loss\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            if step > 1:\n",
    "                average_loss /= display_step\n",
    "            print(\"Step \" + str(step) + \", Average Loss= \" + \\\n",
    "                  \"{:.4f}\".format(average_loss))\n",
    "            average_loss = 0\n",
    "        # evaluation\n",
    "        if step % eval_step == 0 or step == 1:\n",
    "            print(\"Evaluation...\")\n",
    "            sim = sess.run(cosine_sim_op, feed_dict={X: x_test})\n",
    "            for i in range(len(eval_words)):\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = '\"%s\" nearest neighbors:' % eval_words[i]\n",
    "                for k in range(top_k):\n",
    "                    log_str = '%s %s,' % (log_str, id2word[nearest[k]])\n",
    "                print(log_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
