{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TensorFlow eager API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow and debug models, and it reduces boilerplate as well. To follow along with this guide, run the code samples below in an interactive python interpreter.\n",
    "\n",
    "Logistic Regression can be best understood from here: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST handwritten digits  \n",
    "\n",
    "The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "![title](../img/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\supratimdas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eager api imports\n",
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-0b9a51a210b8>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\supratimdas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\supratimdas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\supratimdas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\supratimdas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\supratimdas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator\n",
    "dataset = tf.data.Dataset.from_tensor_slices((mnist.train.images, mnist.train.labels))\n",
    "dataset = dataset.repeat().batch(batch_size).prefetch(batch_size)\n",
    "dataset_Iter = tfe.Iterator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable declarations\n",
    "W = tfe.Variable(tf.zeros([784,10]), name='weights')\n",
    "b = tfe.Variable(tf.zeros([10]), name='biases')\n",
    "\n",
    "# logistic regression input (Wx + b)\n",
    "def logistic_regression(inputs):\n",
    "    return tf.matmul(inputs, W) + b\n",
    "\n",
    "# inference function can be anything like sigmoid, tanH etc. anything\n",
    "def loss_function(inference_fn, inputs, labels):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=inference_fn(inputs),labels=labels))\n",
    "\n",
    "# accuracy function\n",
    "def accuracy_function(inference_fn, inputs, labels):\n",
    "    predictions = tf.nn.softmax(inference_fn(inputs))\n",
    "    correct_preds = tf.equal(tf.argmax(predictions,1), labels)\n",
    "    return tf.reduce_mean(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "gradient = tfe.implicit_gradients(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0001  loss= 2.302585363  accuracy= 0.1094\n",
      "Step: 0100  loss= 0.978982031  accuracy= 0.7995\n",
      "Step: 0200  loss= 0.512349308  accuracy= 0.8890\n",
      "Step: 0300  loss= 0.489089817  accuracy= 0.8807\n",
      "Step: 0400  loss= 0.450997084  accuracy= 0.8902\n",
      "Step: 0500  loss= 0.427731633  accuracy= 0.8948\n",
      "Step: 0600  loss= 0.361523747  accuracy= 0.9133\n",
      "Step: 0700  loss= 0.395773053  accuracy= 0.9023\n",
      "Step: 0800  loss= 0.383722723  accuracy= 0.9032\n",
      "Step: 0900  loss= 0.374225497  accuracy= 0.9069\n",
      "Step: 1000  loss= 0.338724583  accuracy= 0.9170\n"
     ]
    }
   ],
   "source": [
    "# training status variables\n",
    "avg_acc = 0\n",
    "avg_loss = 0\n",
    "\n",
    "# training begins \n",
    "for i in range(epochs):\n",
    "    d = dataset_Iter.next()\n",
    "    x_batch = d[0]\n",
    "    y_batch = tf.cast(d[1], tf.int64)\n",
    "    \n",
    "    # calculating current loss and accuracy\n",
    "    curr_loss = loss_function(logistic_regression, x_batch, y_batch)\n",
    "    curr_acc = accuracy_function(logistic_regression, x_batch, y_batch)\n",
    "    \n",
    "    # adding up the loss and the accuracy\n",
    "    avg_acc += curr_acc\n",
    "    avg_loss += curr_loss\n",
    "    \n",
    "    # optimize the loss (adjust the values of W and b in order to minimize the loss)\n",
    "    optimizer.apply_gradients(gradient(logistic_regression, x_batch, y_batch))\n",
    "    \n",
    "    # print losses\n",
    "    if (i + 1) % display_step == 0 or i == 0:\n",
    "        if i > 0:\n",
    "            avg_acc /= display_step\n",
    "            avg_loss /= display_step\n",
    "        print(\"Step:\", '%04d' % (i + 1), \" loss=\", \"{:.9f}\".format(avg_loss), \" accuracy=\", \"{:.4f}\".format(avg_acc))\n",
    "        average_loss = 0.\n",
    "        average_acc = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9084\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on the test image set\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "test_accuracy = accuracy_function(logistic_regression, x_test, y_test)\n",
    "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
